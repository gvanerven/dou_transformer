{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import spacy\n",
    "import urllib\n",
    "import re\n",
    "import math\n",
    "import torch\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "import random as rd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse('arquivos_xml/12002010410.xml')\n",
    "root = tree.getroot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article\n",
      "numberPage\n",
      "6\n",
      "pubName\n",
      "DO1\n",
      "name\n",
      "PORTARIA\n",
      "artType\n",
      "PORTARIA\n",
      "pubDate\n",
      "04/01/2002\n",
      "artCategory\n",
      "Ministério da Agricultura,/Pecuária e Abastecimento/PROGRAMA DE GARANTIA DA ATIVIDADE AGROPECUÁRIA/COMISSÃO ESPECIAL DE RECURSOS SECRETARIA EXECUTIVA\n",
      "pdfPage\n",
      "http://pesquisa.in.gov.br/imprensa/jsp/visualiza/index.jsp?jornal=1&data=04/01/2002&pagina=6\n",
      "editionNumber\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for child in root:\n",
    "    print(child.tag)\n",
    "    #print(child.root)\n",
    "    for key in child.attrib:\n",
    "        print(key)\n",
    "        print(child.attrib[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'portaria nº 1, de 2 de janeiro de 2002 o secretário-executivo da secretaria da comissão especial de recursos, no uso de sua competência e das atribuições estabelecidas pelas portarias nº 422, de 8 de outubro de 1997, nº 534, de 10 de novembro de 1998, do ministério da agricultura e do abastecimento, divulga, na forma do disposto no art. 3º da resolução nº 2.427, de 1º de outubro de 1997, do banco central do brasil, informações para efeito do zoneamento agrícola, contemplando épocas indicadas para implantação da cultura/custeio de entressafra, cultivares, tipos e aptidão dos solos. art. 1º - cultura temporária: algodão herbáceo. art. 2º - cultura permanente: algodão arbóreo. art. 3º - safra 2002/2003 - estado do rio grande do norte. art. 4º - as pertinentes discriminações estão contidas em anexo, observado o disposto nas resoluções nos  2.403 e 2.422, de 25 de junho e 10 de setembro de 1997, respectivamente, do banco central do brasil. art. 5º- esta portaria entra em vigor na data de sua publicação no diário oficial da união, revogadas as disposições em contrário. luiz antonio rossetti anexo zoneamento agrícola do ministério da agricultura, pecuária e abastecimento rio grande do norte algodão herbáceo e arbóreo safra 2002/2003 portaria nº 1, de 2.1.2002 1. tipos de solos aptos para o plantio algodão herbáceo: solos de caráter eutrófico, pertencentes aos grupos latossolos, podzólicos, brunizen, planossolos, cambissolos, vertissolos, terra rocha estruturada, regossolos e os aluviais, e suas associações. algodão arbóreo: predomínio de solos bruno não cálcicos, litólicos, podzólicos vermelho amarelo, areias quartzosas, planossolos, cambissolos, solonetz solodizado e solonchak, e suas associações. 2. municípios e períodos favoráveis de plantio a relação de municípios aptos para o plantio - suprimidos todos os outros onde a cultura não é recomendada nesta época - foi calcada em dados disponíveis por ocasião da sua elaboração.  se algum município mudou de nome ou foi criado um novo em razão de emancipação de um daqueles da listagem abaixo, todas as recomendações são idênticas às do município de origem até que nova relação o inclua formalmente. a época de plantio indicada pelo zoneamento não será prorrogada ou antecipada em hipótese alguma. no caso de ocorrer algum evento atípico à época indicada  (p.ex.: seca excessiva que impeça o preparo do solo e semeadura ou excesso de chuvas que não permita o tráfego de máquinas na propriedade), recomenda-se aos produtores não efetivarem a implantação da lavoura nesta safra no local atingido, uma vez que, fatalmente, o empreendimento estará sujeito a eventos climáticos adversos impossíveis, ainda, de serem previstos pelo zoneamento.'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'arquivos_xml/12002010410.xml'\n",
    "def read_xml(xml_file_location):\n",
    "    tree = ET.parse(xml_file_location)\n",
    "    root = tree.getroot()\n",
    "    soup = None\n",
    "    docs = ''\n",
    "    for paragraph in root.iter('Texto'):\n",
    "        #print(paragraph.text)\n",
    "        soup = BeautifulSoup(paragraph.text, 'html.parser')\n",
    "        for p in soup.select('p'):\n",
    "            texto = p.text.replace('\\n', ' ').strip()\n",
    "            if len(texto) > 0:\n",
    "                docs = docs + ' ' + texto\n",
    "        docs = docs.strip().lower()\n",
    "    return docs\n",
    "\n",
    "read_xml(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text, spacy_model=None, lemm_alternatives=None, exeption_lst=['ADJ', 'PROPN', 'NOUN'], insert_val_mask=True, debug=False):\n",
    "    last_viewed = None\n",
    "    text_split = re.split(r'(\\W+)', text)\n",
    "    #text_split = re.sub(r'\\W+', ' ', text)\n",
    "    if spacy_model == None:\n",
    "        nlp = spacy.load(\"pt_core_news_sm\")\n",
    "    else:\n",
    "        nlp = spacy_model\n",
    "        \n",
    "    model = nlp(' '.join(text_split).strip())\n",
    "    #model = nlp(text_split.strip())\n",
    "    tokens = []\n",
    "    new_tokens = []\n",
    "    count = 0\n",
    "    for word in model:\n",
    "        str_word = str(word.text).lower().strip()\n",
    "        if str_word != last_viewed:\n",
    "            if count > 0:\n",
    "                tokens += new_tokens + ['##SEQT']\n",
    "            elif last_viewed != None:\n",
    "                tokens += new_tokens\n",
    "                \n",
    "            new_tokens = []\n",
    "            last_viewed = str_word\n",
    "            count = 0\n",
    "            \n",
    "            num = True\n",
    "            len_word = 0\n",
    "            \n",
    "            if lemm_alternatives and str_word in lemm_alt:\n",
    "                    str_lemma = lemm_alternatives[str_word].lower()\n",
    "            else:\n",
    "                str_lemma = str(word.lemma_).lower().strip()\n",
    "                \n",
    "            try:\n",
    "                int_word = int(str_word)\n",
    "            except:\n",
    "                num = False\n",
    "\n",
    "            if num:\n",
    "                new_tokens.append('##NUM')\n",
    "                if insert_val_mask:\n",
    "                    new_tokens.append(str_word)\n",
    "                if debug:\n",
    "                    print(str_word)\n",
    "                    print(f'num: {word.pos_}')\n",
    "            elif word.pos_ == 'NUM':\n",
    "                new_tokens.append('##NUMEXT')\n",
    "                if insert_val_mask:\n",
    "                    new_tokens.append(str_word)\n",
    "                if debug:\n",
    "                    print(str_word)\n",
    "                    print(f'num: {word.pos_}')\n",
    "            elif word.pos_ in exeption_lst:\n",
    "                if debug:\n",
    "                    print(str_word)\n",
    "                    print(f'in exeption list: {word.pos_}')\n",
    "                tokens.append(str_word)\n",
    "            elif len(str_word) > 0 and str_word == str_lemma:\n",
    "                new_tokens.append(str_word)\n",
    "            else:\n",
    "                #lemma\n",
    "                i = 0\n",
    "                \n",
    "                if len(str_lemma) < len(str_word):\n",
    "                    len_word = len(str_lemma)\n",
    "                else:\n",
    "                    len_word = len(str_word)\n",
    "\n",
    "                while i < len_word and str_word[i] == str_lemma[i]:\n",
    "                    i += 1\n",
    "                #print(str_word, str_lemma)\n",
    "                if i == 0 and len(str_word) > 0:\n",
    "                    new_tokens.append(str_word)              \n",
    "                    new_tokens.append('##' + str_lemma)\n",
    "                elif len(str_word[:i].strip()) > 0:\n",
    "                    new_tokens.append(str_word[:i].strip())\n",
    "\n",
    "                if i != 0 and len(str_word[i:].strip()) > 0:\n",
    "                    new_tokens.append('##' + str_word[i:].strip())\n",
    "\n",
    "                if debug:\n",
    "                    print(i)\n",
    "                    print(str_word)\n",
    "                    print(str_lemma[0:i], str_lemma[i:])\n",
    "                    print(word.pos_)\n",
    "        else:\n",
    "            count += 1\n",
    "                \n",
    "    return tokens + new_tokens\n",
    "    \n",
    "def pack_tokens(tokens, dictionary, max_length= 512):\n",
    "    nr_sep = math.ceil(len(tokens)/max_length)\n",
    "    max_length_sep = max_length - nr_sep\n",
    "    blks = math.ceil(len(tokens)/max_length_sep)\n",
    "    inputs = []\n",
    "    blocks = []\n",
    "    masks = []\n",
    "    dictionary.add_documents([tokens])\n",
    "    for i in range(blks):\n",
    "        mask = [1] * max_length\n",
    "        offset = max_length_sep*i\n",
    "        #print(len(tokens[offset:(max_length_sep + offset)]))\n",
    "        res = [\"[CLS]\"] + tokens[offset:(max_length_sep + offset)] + [\"[SEP]\"]\n",
    "        if len(res) < max_length:\n",
    "            for i in range(len(res), max_length):\n",
    "                mask[i] = 0\n",
    "                res.append('[PAD]')\n",
    "        #print(len(res))\n",
    "        inputs.append(mask_block(res, dictionary))\n",
    "        blocks.append(res)\n",
    "        masks.append(mask)\n",
    "    return (inputs, blocks, masks)\n",
    "\n",
    "def mask_block(block, dictionary=None):\n",
    "    prob = rd.random()\n",
    "    if prob <= 0.1:\n",
    "        return block\n",
    "    else:\n",
    "        mask_pos = rd.randint(1, block.index(\"[SEP]\"))\n",
    "        print(mask_pos)\n",
    "        if block[mask_pos] not in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
    "            if prob <= 0.2:\n",
    "                if dictionary == None:\n",
    "                    mask_pos2 = rd.randint(1, block.index(\"[SEP]\")-1)\n",
    "                    if mask_pos2 != mask_pos:\n",
    "                        return block[0:mask_pos] + block[mask_pos2] + block[mask_pos+1:]\n",
    "                    else:\n",
    "                        return block[0:mask_pos] + block[mask_pos2+1] + block[mask_pos+1:]\n",
    "                else:\n",
    "                    mask_pos2 = rd.randint(4, len(dictionary)-1)\n",
    "                    if dictionary.token2id[block[mask_pos2]] != mask_pos:\n",
    "                        return block[0:mask_pos] + [dictionary[mask_pos2]] + block[mask_pos+1:]\n",
    "                    else:\n",
    "                        return block[0:mask_pos] + [dictionary[mask_pos2+1]] + block[mask_pos+1:]\n",
    "            else:\n",
    "                return block[0:mask_pos] + ['[MASK]'] + block[mask_pos+1:]\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK]'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms[310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cambissolos'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packs[0][0][310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310\n"
     ]
    }
   ],
   "source": [
    "ms = mask_block(packs[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "3\n",
      "512\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary()\n",
    "dictionary.add_documents([[\"[CLS]\"], [\"[SEP]\"], [\"[MASK]\"], [\"[PAD]\"]])\n",
    "packs = pack_tokens(tokenizer(docs, insert_val_mask=False), dictionary)\n",
    "print(len(packs))\n",
    "print(len(packs[0][0]))\n",
    "print(len(packs[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'entressafra'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packs[1][0][118]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "dictionary = corpora.Dictionary()\n",
    "dictionary.add_documents([[\"[CLS]\"], [\"[SEP]\"], [\"[MASK]\"], [\"[PAD]\"]])\n",
    "\n",
    "for texto in docs:\n",
    "    #print(\"Processing:\", len(texto))\n",
    "    tk = tokenizer(el, spacy_model=nlp, lemm_alternatives=lemm_alt, insert_val_mask=False, debug=False)\n",
    "    res, ms = pack_tokens(tk)\n",
    "    dictionary.add_documents(res)\n",
    "    for i, r in enumerate(res):\n",
    "        dataset.append({\"doc\": r, \"mask\": ms[i]})\n",
    "            \n",
    "for item in dataset:\n",
    "    elems = []\n",
    "    for el in item[\"doc\"]:\n",
    "        elems.append(dictionary.token2id[el])\n",
    "    item[\"encoded\"] = elems\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id[dictionary[len(dictionary)-1]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}